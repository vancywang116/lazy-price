{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of Lazy Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic prep: measuring the document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "import nltk\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demand</th>\n",
       "      <th>expect</th>\n",
       "      <th>in</th>\n",
       "      <th>increase</th>\n",
       "      <th>sales</th>\n",
       "      <th>to</th>\n",
       "      <th>we</th>\n",
       "      <th>weakness</th>\n",
       "      <th>worldwide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   demand  expect  in  increase  sales  to  we  weakness  worldwide\n",
       "0       1       1   0         1      0   1   1         0          0\n",
       "1       1       1   0         1      0   1   1         0          1\n",
       "2       0       1   1         0      1   0   1         1          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = [\"We expect demand to increase.\",\n",
    "      \"We expect worldwide demand to increase.\",\n",
    "      \"We expect weakness in sales.\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df)\n",
    "vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the first two documents: [[0.91287093]]\n"
     ]
    }
   ],
   "source": [
    "similarity = cosine_similarity(X[0], X[1])\n",
    "print(\"Cosine similarity between the first two documents:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the first and third documents: [[0.4]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cosine similarity between the first and third documents:\", cosine_similarity(X[0], X[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard similarity between the first two documents: 0.8333333333333334\n",
      "Jaccard similarity between the first and third documents: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Convert the frequency counts to binary format\n",
    "X_binary = (X.toarray() > 0).astype(int)\n",
    "print(\"Jaccard similarity between the first two documents:\", jaccard_score(X_binary[0],X_binary[1]))\n",
    "print(\"Jaccard similarity between the first and third documents:\", jaccard_score(X_binary[0],X_binary[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit distance between the first two documents: 1\n"
     ]
    }
   ],
   "source": [
    "# Convert the first two rows to dense format\n",
    "doc1_vector = X.toarray()[0]\n",
    "doc2_vector = X.toarray()[1]\n",
    "\n",
    "features = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Expand each feature name by its frequency to simulate \"words\"\n",
    "doc1_expanded = []\n",
    "doc2_expanded = []\n",
    "for word, count in zip(features, doc1_vector):\n",
    "    doc1_expanded.extend([word] * count)\n",
    "for word, count in zip(features, doc2_vector):\n",
    "    doc2_expanded.extend([word] * count)\n",
    "\n",
    "# Calculate the edit distance between the two expanded lists\n",
    "distance = edit_distance(doc1_expanded, doc2_expanded)\n",
    "\n",
    "print(\"Edit distance between the first two documents:\", distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the word count in each document, which is more complex than it might initially appear.\n",
    "\n",
    "First, it's unclear which parts of the document we should count. For example, the initial lines in the original filings are not typically considered content. Additionally, the word count results can vary significantly depending on the tokenizer used.\n",
    "\n",
    "Therefore, we plan to calculate the word count in several ways:\n",
    "\n",
    "1. Count words in the entire filings using a whitespace tokenizer.\n",
    "2. Count words in the entire filings using the TreebankWordTokenizer.\n",
    "3. Count words in the Management Discussion and Analysis (MD&A) section using a whitespace tokenizer.\n",
    "4. Count words in the MD&A section using the TreebankWordTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Example for 10-K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Local_directory/10-X_C_1993-2000/1995/QTR1/19950103_10-K_edgar_data_44471_0000044471-95-000002.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it out\n",
    "# print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count words in the entire filings using a whitespace tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25617"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = 0\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        words = line.split()\n",
    "        word_count += len(words)\n",
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count words in the entire filings using the TreebankWordTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30044"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "treebank_word_count = 0\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    # Tokenize the content using the Penn Treebank Tokenizer\n",
    "    words = tokenizer.tokenize(content)\n",
    "    treebank_word_count = len(words)\n",
    "treebank_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count words in the Management Discussion and Analysis (MD&A) section using a whitespace tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrape_yu_z import get_itemized_10k, get_itemized_10q \n",
    "temp = get_itemized_10k(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp['mda'].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count words in the MD&A section using the TreebankWordTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(temp['mda']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Example for 10-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Local_directory/10-X_C_1993-2000/1995/QTR1/19950103_10-Q-A_edgar_data_356226_0000356226-95-000001.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = 0\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        words = line.split()\n",
    "        word_count += len(words)\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "treebank_word_count = 0\n",
    "with open(file_path, 'r') as file:\n",
    "    content = file.read()\n",
    "    # Tokenize the content using the Penn Treebank Tokenizer\n",
    "    words = tokenizer.tokenize(content)\n",
    "    treebank_word_count = len(words)\n",
    "treebank_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mda': None}\n"
     ]
    }
   ],
   "source": [
    "temp = get_itemized_10q(file_path)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to take this case account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count_functions(input_file):\n",
    "    with open(input_file, 'r') as file:\n",
    "        content = file.read()\n",
    "        simple_word_count = len(content.split())\n",
    "        treebank_word_count = len(tokenizer.tokenize(content))\n",
    "    # use different function depending on the filings\n",
    "    if '10-K' in input_file:\n",
    "        try: \n",
    "            mda_content = get_itemized_10k(input_file)['mda']\n",
    "        except:\n",
    "            pass\n",
    "    else:\n",
    "        try: \n",
    "            mda_content = get_itemized_10q(input_file)['mda']\n",
    "        except:\n",
    "            pass\n",
    "    try:\n",
    "        simple_mda = len(mda_content.split())\n",
    "    except:\n",
    "        simple_mda = 0 # use 0 as a wildcard\n",
    "    try :\n",
    "        treebank_mda = len(tokenizer.tokenize(mda_content))\n",
    "    except:\n",
    "        treebank_mda = 0 # use 0 as a wildcard\n",
    "        \n",
    "    return {'filing_date':input_file.split('/')[-1].split('_')[0],\n",
    "            'CIK':input_file.split('/')[-1].split('_')[-1].split('-')[0],\n",
    "            'category':input_file.split('/')[-1].split('_')[1],\n",
    "            'simple_word_count':simple_word_count,\n",
    "            'treebank_word_count':treebank_word_count,\n",
    "            'simple_mda':simple_mda,\n",
    "            'treebank_mda':treebank_mda}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filing_date</th>\n",
       "      <th>CIK</th>\n",
       "      <th>category</th>\n",
       "      <th>simple_word_count</th>\n",
       "      <th>treebank_word_count</th>\n",
       "      <th>simple_mda</th>\n",
       "      <th>treebank_mda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19950103</td>\n",
       "      <td>0000356226</td>\n",
       "      <td>10-Q-A</td>\n",
       "      <td>413</td>\n",
       "      <td>568</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filing_date         CIK category  simple_word_count  treebank_word_count  \\\n",
       "0    19950103  0000356226   10-Q-A                413                  568   \n",
       "\n",
       "   simple_mda  treebank_mda  \n",
       "0           0             0  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([word_count_functions(file_path)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 10-X from 1995 to 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = '/Local_directory/10-X_C_1993-2000/'\n",
    "years = ['1995','1996','1997','1998','1999','2000']\n",
    "quarters = ['QTR1','QTR2','QTR3','QTR4']\n",
    "\n",
    "#files = [f for f in os.listdir(folder_path+'QTR1') if os.path.isfile(os.path.join(folder_path+'QTR1', f))]\n",
    "#print(\"Files in '\", folder_path+'QTR1', \"':\")\n",
    "#k_files = [item for item in files if 'K' in item] \n",
    "#q_files = [item for item in files if 'Q' in item]\n",
    "#print(k_files[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    df = pd.DataFrame()\n",
    "    for quarter in quarters:\n",
    "        file_path = folder_path + year + '/' + quarter\n",
    "        filings = os.listdir(file_path)\n",
    "        for filing in filings:\n",
    "            temp = pd.DataFrame([word_count_functions(file_path + '/' + filing)])\n",
    "            df = pd.concat([df,temp],ignore_index=True)\n",
    "        print(year, quarter)\n",
    "    df.to_csv('Local_directory/julex_data/'+year+'.csv.gz',\n",
    "              compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
